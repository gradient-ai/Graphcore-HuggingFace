{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Automated Speech Recognition (ASR) using a fine-tuned wav2vec 2.0 checkpoint on IPU\n",
    "\n",
    "This notebook will demonstrate how to perform wav2vec 2.0 inference with PyTorch on the Graphcore IPUs. We will use a `wav2vec2-base` model fine-tuned for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ðŸ¤—`transformers` library from HuggingFace and paralllize it using the ðŸ¤—`optimum-graphcore` library.\n",
    "\n",
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/gradient-ai/Graphcore-HuggingFace/issues).\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt update\n",
    "apt-get install libsndfile1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import poptorch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from transformers import (\n",
    "    AutoModelForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod4\")\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/wav2vec2_inference\"\n",
    "checkpoint_directory = Path(os.getenv(\"PERSISTENT_CHECKPOINT_DIR\", \"/tmp\")) / \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the model\n",
    "\n",
    "This notebook uses the model output from the fine-tuning notebook. If you have not run the fine-tuning notebook, or don't have an output directory, then this script will not run.\n",
    "\n",
    "As this model does not require optimising, the full `base` inference model can fit on a single IPU. This makes the IPU configuration straightforward. The `num_device_iterations` will control how many iterations the IPU will perform before returning to host. With this set to 10, 10 utterances will be sent to the IPU, processed, and sent back as a block of 10. \n",
    "\n",
    "We create the pipelined version of the model which makes changes for the IPU version of the model. And finally convert the model into a `poptorch.inferenceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(checkpoint_directory)\n",
    "model = AutoModelForCTC.from_pretrained(checkpoint_directory)\n",
    "\n",
    "num_device_iterations = 10\n",
    "ipu_config = IPUConfig(inference_device_iterations=num_device_iterations, executable_cache_dir=executable_cache_dir)\n",
    "opts = ipu_config.to_options(for_inference=True)\n",
    "\n",
    "ipu_model = to_pipelined(model, ipu_config)\n",
    "ipu_model.parallelize()\n",
    "\n",
    "inference_model = poptorch.inferenceModel(ipu_model.half().eval(), options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "The sample batch is an example of what a batch could look like. Effectively, we are setting the static size for the model input. The first dimension is the product of the `batch_size` and `num_device_iterations`. However, in this case the batch size is just 1. The second dimension is the maximum audio length in samples. We've set this to 20 seconds.\n",
    "\n",
    "The model will then compile for this input size. If the size is changed later the model will recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 400000\n",
    "sample_batch = {\"input_values\": torch.zeros([num_device_iterations, max_samples], dtype=torch.half)}\n",
    "\n",
    "inference_model.compile(**sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibriSpeech inference\n",
    "\n",
    "We will test the inference capabilities of a fine-tuned model on a portion of the LibriSpeech `test` split. First, download the dataset using the ðŸ¤—`datasets` library from HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a batch\n",
    "\n",
    "Here we take examples from LibriSpeech test and place them into a `zeros` tensor to create a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros([num_device_iterations, max_samples], dtype=torch.half)\n",
    "\n",
    "for i in range(num_device_iterations):\n",
    "    input_values = processor(\n",
    "        ds[i][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\"\n",
    "    ).input_values  # Batch size 1\n",
    "    length = input_values.size(1)\n",
    "    x[i, :length] = input_values[0]\n",
    "\n",
    "batch = {\"input_values\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Running the model will perform `num_device_iterations` on the IPU before returning to host. This means that all of our logits will be returned at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inference_model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode\n",
    "\n",
    "The max arg of the logits is taked from every frame of the output, this is a 'greedy decode' strategy. The processor will then convert the predicted indexes back into text, and the transcripts will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output[0]\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: release IPUs in use\n",
    "\n",
    "The IPython kernel has a lock on the IPUs used in running the model, preventing other users from using them. For example, if you wish to use other notebooks after working your way through this one, it may be necessary to manually run the following cell to release IPUs from use. This will happen by default if using the `Run All` option. More information on the topic can be found at [Managing IPU Resources](https://github.com/gradient-ai/Graphcore-HuggingFace/blob/main/useful-tips/managing_ipu_resources.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference_model.isAttachedToDevice():\n",
    "    inference_model.detachFromDevice()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
