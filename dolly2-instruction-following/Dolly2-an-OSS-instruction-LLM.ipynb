{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b361bf",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af14ce5a",
   "metadata": {},
   "source": [
    "# Dolly 2.0 – The World’s First, Truly Open Instruction-Tuned LLM on IPUs – Inference\n",
    "\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "|   NLP   |  Instruction Fine-tuned Text Generation  | Dolly 2.0 | N/A | Inference | recommended: 16 (minimum 4) |  ???   |\n",
    "\n",
    "[Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) is a 12B parameter language model trained and instruction fine-tuned by [Databricks](https://www.databricks.com). By instruction fine-tuning the large language model (LLM), we obtain an LLM better suited for human interactivity. Crucially, Databricks released all code, model weights, and their fine-tuning dataset with an open-source license that permits commercial use. This makes Dolly 2.0 the world's first, truly open-source instruction-tuned LLM. In this notebook, we will show you how to run Dolly 2.0 using Graphcore IPUs on Paperspace with your own prompts.\n",
    "\n",
    "In this notebook you will:\n",
    "- Create and configure a Dolly inference pipeline.\n",
    "- Run Dolly inference on a text prompts to generate answers to user specified questions.\n",
    "\n",
    "This notebook requires a minimum of 4 IPUs to run. This notebook also supports running on a POD16, resulting in a faster pipeline inference speed.\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db890f52",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "<!-- TODO: add a run on gradient button. -->\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "[![Run on Gradient](../../gradient-badge.svg)]()\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine.\n",
    "\n",
    "Run the next cell to install extra requirements for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965ca0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "number_of_ipus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa108da",
   "metadata": {},
   "source": [
    "## Dolly inference pipeline\n",
    "\n",
    "Let's begin by loading the inference config for Dolly. A configuration suitable for your instance will automatically be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5aa39b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:21:46 INFO: Starting. Process id: 49350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DollyConfig(model=ModelConfig(layers=36, hidden_size=5120, sequence_length=2048, precision=<Precision.float16: popxl.dtypes.float16>, seed=42, embedding=ModelConfig.Embedding(vocab_size=50280), attention=ModelConfig.Attention(heads=40, rotary_positional_embeddings_base=10000, rotary_dim=32)), execution=Execution(micro_batch_size=1, data_parallel=1, device_iterations=1, io_tiles=1, available_memory_proportion=(0.4,), tensor_parallel=16, attention_tensor_parallel=8, code_load=False))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.setup import dolly_config_setup\n",
    "\n",
    "config_name = \"dolly_pod4\" if number_of_ipus == 4 else \"dolly_pod16\"\n",
    "config, *_ = dolly_config_setup(\"config/inference.yml\", \"release\", config_name)\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e65adf1",
   "metadata": {},
   "source": [
    "Next, we want to create our inference pipeline. Here we define the maximum\n",
    "sequence length. Before executing a model on IPUs it needs to be turned into an\n",
    "executable format by compiling it. This will happen when the pipeline is\n",
    "created. All input shapes must be known before compiling, so if the maximum\n",
    "sequence length is changed, the pipeline will need to be recompiled.\n",
    "\n",
    "*This cell will take approximately 18 minutes to complete, which includes downloading the model weights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21144e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:21:53 INFO: Creating session\n",
      "2023-04-27 10:21:53 INFO: Starting PopXL IR construction\n",
      "2023-04-27 10:22:26 INFO: PopXL IR construction duration: 0.55 mins\n",
      "2023-04-27 10:22:26 INFO: PopXL IR construction complete\n",
      "2023-04-27 10:22:26 INFO: Starting PopXL compilation\n",
      "2023-04-27 10:23:28 INFO: PopXL compilation duration: 1.03 mins\n",
      "2023-04-27 10:23:28 INFO: PopXL compilation complete\n",
      "2023-04-27 10:23:28 INFO: Downloading 'databricks/dolly-v2-12b' pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"23551\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 10:25:52 INFO: Completed pretrained weights download.\n",
      "2023-04-27 10:25:52 INFO: Downloading 'databricks/dolly-v2-12b' tokenizer\n",
      "2023-04-27 10:25:52 INFO: Completed tokenizer download.\n",
      "2023-04-27 10:25:52 INFO: Starting Loading HF pretrained model to IPU\n",
      "2023-04-27 10:27:47 INFO: Loading HF pretrained model to IPU duration: 1.91 mins\n",
      "2023-04-27 10:27:47 INFO: IPU pretrained weights loading complete.\n",
      "2023-04-27 10:27:47 INFO: Finished initialising pipeline\n"
     ]
    }
   ],
   "source": [
    "import api\n",
    "\n",
    "# changing these parameters will trigger a recompile\n",
    "sequence_length = 256  # max 2048\n",
    "\n",
    "dolly_pipeline = api.DollyPipeline(\n",
    "    config,\n",
    "    sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5ae263d",
   "metadata": {},
   "source": [
    "And run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e87e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 12:09:49 INFO: Attach to IPUs\n",
      "2023-04-27 12:09:49 INFO: Start inference\n",
      "2023-04-27 12:09:49 INFO: Input prompt: How many islands are there in Scotland?\n",
      "2023-04-27 12:09:49 INFO: Response:\n",
      "There are 732 islands off the coast of mainland Scotland, with the largest being Rùm (pronounced “roo”) – which is home to over 100,000 sheep. St Kilda is the most remote inhabited island, and is now a nature reserve.\n",
      "\n",
      "\n",
      "2023-04-27 12:10:15 INFO: Output in 25.25 seconds\n",
      "2023-04-27 12:10:15 INFO: Throughput: 2.22 t/s\n"
     ]
    }
   ],
   "source": [
    "answer = dolly_pipeline(\"How many islands are there in Scotland?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8e787be",
   "metadata": {},
   "source": [
    "We've just ran Dolly with the default parameters. Thanks to instruction fine-tuning, we can give prompts to Dolly as if it were a chatbot, as instruction fine-tuning helps turn a base language model into one more suited for interactive behaviour with humans.\n",
    "\n",
    "There are a few sampling parameters we can use to control the behaviour of Dolly:\n",
    "- `temperature` – Indicates whether you want more creative or more factual outputs. A higher value generates more creative outputs and a lower value generates more factual answers. Typical values fall between `0.0` and `1.0`.\n",
    "- `top_k` – Indicates that only among the highest `top_k` probable tokens should be sampled. Set to 0 to sample across all possible tokens, which means that top k sampling is disabled. The value for `top_k` must be between a minimum of 0 and a maximum of `config.model.embedding.vocab_size` which is 50,280.\n",
    "- `output_length` – Indicates the number of tokens to sample before stopping. Sampling can stop early if the model outputs `### END`.\n",
    "- `print_final` – If `True`, prints the total time and throughput.\n",
    "- `print_live` – If `True`, the tokens will be printed as they are being sampled. If  `False`, only the answer will be returned as a string.\n",
    "- `prompt` – A string containing the question you wish to generate an answer for.\n",
    "\n",
    "Changing these parameters will not result in any recompilation as the input shape to the model has not changed. These can be freely changed and experimented with in the next cell to produce the behaviour you want from Dolly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9693f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 12:16:59 INFO: Attach to IPUs\n",
      "2023-04-27 12:16:59 INFO: Start inference\n",
      "2023-04-27 12:16:59 INFO: Input prompt: Who was Dolly the sheep?\n",
      "2023-04-27 12:16:59 INFO: Response:\n",
      "Dolly the sheep was a sheep that was the first successfully cloned mammal\n",
      "\n",
      "\n",
      "2023-04-27 12:17:07 INFO: Output in 7.54 seconds\n",
      "2023-04-27 12:17:07 INFO: Throughput: 2.12 t/s\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.6\n",
    "top_k = 5\n",
    "output_length = None\n",
    "print_live = True\n",
    "print_final = True\n",
    "\n",
    "prompt = \"Who was Dolly the sheep?\"\n",
    "answer = dolly_pipeline(\n",
    "    prompt,\n",
    "    temperature=temperature,\n",
    "    k=top_k,\n",
    "    output_length=output_length,\n",
    "    print_live=print_live,\n",
    "    print_final=print_final,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0a7a7cc",
   "metadata": {},
   "source": [
    "As Dolly is an instruction fine-tuned model, it was trained with a specific prompt format. Internally, the pipeline will transform your prompts into the correct format. To see the full prompt, you can view the `last_instruction_prompt` attribute on the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85d0409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instruction below describes a task. Write a response that appropriately completes the request.\n",
      "    ### Instruction:\n",
      "    Who was Dolly the sheep?\n",
      "    ### Response:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(dolly_pipeline.last_instruction_prompt[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ca473c",
   "metadata": {},
   "source": [
    "Remember to detach your pipeline when you are finished to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e91b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_pipeline.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ece4041",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how you can easily run Dolly 2.0 for inference on Graphcore IPUs on text prompts. Instruction fine-tuning is a powerful method of turning a base LLM into one more suited for human interactivity, such as a question-answer model or a chatbot.\n",
    "\n",
    "Although larger instruction LLMs exist with more world knowledge such as ChatGPT, they are closed-source or are subject to non-commercial licensing. This makes Dolly 2.0 a significant milestone as the first of its kind, with future instruction fine-tuned LLMs no doubt to quickly follow. All of which will be truly open."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73e9821a",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Check out the full list of [IPU-powered Jupyter Notebooks](https://www.graphcore.ai/ipu-jupyter-notebooks) to get more of a feel for how IPUs perform on other tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
