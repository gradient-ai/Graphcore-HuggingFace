{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b361bf",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af14ce5a",
   "metadata": {},
   "source": [
    "# Dolly 2.0 – The World’s First, Truly Open Instruction-Tuned LLM on IPUs – Inference\n",
    "\n",
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "|   NLP   |  Instruction Fine-tuned Text Generation  | Dolly 2.0 | N/A | Inference | recommended: 16 (minimum 4) |  ???   |\n",
    "\n",
    "[Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) is a 12B parameter language model trained and instruction fine-tuned by [Databricks](https://www.databricks.com). By instruction fine-tuning the large language model (LLM), we obtain an LLM better suited for human interactivity. Crucially, Databricks released all code, model weights, and their fine-tuning dataset with an open-source license that permits commercial use. This makes Dolly 2.0 the world's first, truly open-source instruction-tuned LLM. In this notebook, we will show you how to run Dolly 2.0 using Graphcore IPUs on Paperspace with your own prompts.\n",
    "\n",
    "In this notebook you will:\n",
    "- Create and configure a Dolly inference pipeline.\n",
    "- Run Dolly inference on a text prompts to generate answers to user specified questions.\n",
    "\n",
    "This notebook requires a minimum of 4 IPUs to run. This notebook also supports running on a POD16, resulting in a faster pipeline inference speed.\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db890f52",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "<!-- TODO: add a run on gradient button. -->\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "[![Run on Gradient](../../gradient-badge.svg)]()\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine.\n",
    "\n",
    "Run the next cell to install extra requirements for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965ca0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", 16))\n",
    "number_of_ipus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa108da",
   "metadata": {},
   "source": [
    "## Dolly inference pipeline\n",
    "\n",
    "Let's begin by loading the inference config for Dolly. A configuration suitable for your instance will automatically be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5aa39b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 14:10:10 WARNING: Dropping extra args {'attention_tensor_parallel': 8}\n",
      "2023-04-28 14:10:10 INFO: Starting. Process id: 117568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DollyConfig(model=ModelConfig(layers=36, hidden_size=5120, sequence_length=2048, precision=<Precision.float16: popxl.dtypes.float16>, seed=42, embedding=ModelConfig.Embedding(vocab_size=50280), attention=ModelConfig.Attention(heads=40, rotary_positional_embeddings_base=10000, rotary_dim=32)), execution=Execution(micro_batch_size=1, data_parallel=1, device_iterations=1, io_tiles=1, available_memory_proportion=(0.4,), tensor_parallel=16, code_load=False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.setup import dolly_config_setup\n",
    "\n",
    "config_name = \"dolly_pod4\" if number_of_ipus == 4 else \"dolly_pod16\"\n",
    "config, *_ = dolly_config_setup(\"config/inference.yml\", \"release\", config_name)\n",
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e65adf1",
   "metadata": {},
   "source": [
    "Next, we want to create our inference pipeline. Here we define the maximum\n",
    "sequence length. Before executing a model on IPUs it needs to be turned into an\n",
    "executable format by compiling it. This will happen when the pipeline is\n",
    "created. All input shapes must be known before compiling, so if the maximum\n",
    "sequence length is changed, the pipeline will need to be recompiled.\n",
    "\n",
    "*This cell will take approximately 18 minutes to complete, which includes downloading the model weights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21144e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 14:10:12 INFO: Creating session\n",
      "2023-04-28 14:10:12 INFO: Starting PopXL IR construction\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "40 % 16 != 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# changing these parameters will trigger a recompile\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sequence_length \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m  \u001b[39m# max 2048\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dolly_pipeline \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mDollyPipeline(\n\u001b[1;32m      7\u001b[0m     config,\n\u001b[1;32m      8\u001b[0m     sequence_length\u001b[39m=\u001b[39;49msequence_length,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/Graphcore-HuggingFace-fork/dolly2-instruction-following/api/pipeline.py:106\u001b[0m, in \u001b[0;36mDollyPipeline.__init__\u001b[0;34m(self, config, hf_dolly_checkpoint, sequence_length, micro_batch_size, tokenizer, prompt_format, end_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m# TODO: we really want to lazily compile session, however currently in PopXL we cannot detect globally whether IPU is already attached, making it possible to lock up the notebook kernel. Therefore, we compile in __init__ for now.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCreating session\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m session: popxl\u001b[39m.\u001b[39mSession \u001b[39m=\u001b[39m inference(config)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(hf_dolly_checkpoint, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    108\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhf_dolly_checkpoint\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m pretrained weights\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Graphcore-HuggingFace-fork/dolly2-instruction-following/inference.py:70\u001b[0m, in \u001b[0;36minference\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m# ----- Build compute graphs -----\u001b[39;00m\n\u001b[1;32m     69\u001b[0m embeddings_facts, embeddings_graph \u001b[39m=\u001b[39m DollyEmbeddingsTP(config)\u001b[39m.\u001b[39mcreate_graph(input_streams\u001b[39m.\u001b[39mwords\u001b[39m.\u001b[39mspec)\n\u001b[0;32m---> 70\u001b[0m layer_facts, layer_graph \u001b[39m=\u001b[39m DollyDecoderBlockTP(config)\u001b[39m.\u001b[39mcreate_graph(\u001b[39m*\u001b[39membeddings_graph\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m     71\u001b[0m lm_facts, lm_graph \u001b[39m=\u001b[39m DollyLMHeadTP(config)\u001b[39m.\u001b[39mcreate_graph(layer_graph\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     72\u001b[0m \u001b[39m# ---- Transform graphs ----\u001b[39;00m\n",
      "File \u001b[0;32m~/Graphcore-HuggingFace-fork/dolly2-instruction-following/modelling/decoder.py:29\u001b[0m, in \u001b[0;36mDollyDecoderBlockTP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2 \u001b[39m=\u001b[39m LayerNorm()\n\u001b[1;32m     27\u001b[0m \u001b[39m# attention is sharded\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# identical computation for bias and skip connection\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m DollySelfAttentionTP(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n\u001b[1;32m     30\u001b[0m \u001b[39m# begins with identical computations: layer norm ln_2\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# feed forward is sharded\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# identical computation for bias, dropout and skip connection\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward \u001b[39m=\u001b[39m DollyFeedForwardTP(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/Graphcore-HuggingFace-fork/dolly2-instruction-following/modelling/attention.py:106\u001b[0m, in \u001b[0;36mDollySelfAttentionTP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplica_grouping \u001b[39m=\u001b[39m popxl\u001b[39m.\u001b[39mgcg()\u001b[39m.\u001b[39mir\u001b[39m.\u001b[39mreplica_grouping(stride\u001b[39m=\u001b[39mtp, group_size\u001b[39m=\u001b[39mdp)\n\u001b[1;32m    105\u001b[0m \u001b[39m# Sharded across devices\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads \u001b[39m=\u001b[39m DollyAttentionHeads(config\u001b[39m=\u001b[39;49mconfig, replica_grouping\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplica_grouping)\n\u001b[1;32m    108\u001b[0m \u001b[39m# Sharded across devices\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m Linear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mhidden_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, replica_grouping\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplica_grouping)\n",
      "File \u001b[0;32m~/Graphcore-HuggingFace-fork/dolly2-instruction-following/modelling/attention.py:42\u001b[0m, in \u001b[0;36mDollyAttentionHeads.__init__\u001b[0;34m(self, config, replica_grouping)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     n_heads_groups \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mheads \u001b[39m%\u001b[39m n_heads_groups \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     44\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mheads\u001b[39m}\u001b[39;00m\u001b[39m % \u001b[39m\u001b[39m{\u001b[39;00mn_heads_groups\u001b[39m}\u001b[39;00m\u001b[39m != 0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads_groups \u001b[39m=\u001b[39m n_heads_groups\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mheads \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_heads_groups\n",
      "\u001b[0;31mAssertionError\u001b[0m: 40 % 16 != 0"
     ]
    }
   ],
   "source": [
    "import api\n",
    "\n",
    "# changing these parameters will trigger a recompile\n",
    "sequence_length = 256  # max 2048\n",
    "\n",
    "dolly_pipeline = api.DollyPipeline(\n",
    "    config,\n",
    "    sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5ae263d",
   "metadata": {},
   "source": [
    "And run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e87e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 12:09:49 INFO: Attach to IPUs\n",
      "2023-04-27 12:09:49 INFO: Start inference\n",
      "2023-04-27 12:09:49 INFO: Input prompt: How many islands are there in Scotland?\n",
      "2023-04-27 12:09:49 INFO: Response:\n",
      "There are 732 islands off the coast of mainland Scotland, with the largest being Rùm (pronounced “roo”) – which is home to over 100,000 sheep. St Kilda is the most remote inhabited island, and is now a nature reserve.\n",
      "\n",
      "\n",
      "2023-04-27 12:10:15 INFO: Output in 25.25 seconds\n",
      "2023-04-27 12:10:15 INFO: Throughput: 2.22 t/s\n"
     ]
    }
   ],
   "source": [
    "answer = dolly_pipeline(\"How many islands are there in Scotland?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8e787be",
   "metadata": {},
   "source": [
    "We've just ran Dolly with the default parameters. Thanks to instruction fine-tuning, we can give prompts to Dolly as if it were a chatbot, as instruction fine-tuning helps turn a base language model into one more suited for interactive behaviour with humans.\n",
    "\n",
    "There are a few sampling parameters we can use to control the behaviour of Dolly:\n",
    "- `temperature` – Indicates whether you want more creative or more factual outputs. A higher value generates more creative outputs and a lower value generates more factual answers. Typical values fall between `0.0` and `1.0`.\n",
    "- `top_k` – Indicates that only among the highest `top_k` probable tokens should be sampled. Set to 0 to sample across all possible tokens, which means that top k sampling is disabled. The value for `top_k` must be between a minimum of 0 and a maximum of `config.model.embedding.vocab_size` which is 50,280.\n",
    "- `output_length` – Indicates the number of tokens to sample before stopping. Sampling can stop early if the model outputs `### END`.\n",
    "- `print_info` – If `True`, prints extra information such as the total time taken and throughput.\n",
    "- `print_live` – If `True`, the tokens will be printed as they are being sampled. If  `False`, only the answer will be returned as a string.\n",
    "- `prompt` – A string containing the question you wish to generate an answer for.\n",
    "\n",
    "Changing these parameters will not result in any recompilation as the input shape to the model has not changed. These can be freely changed and experimented with in the next cell to produce the behaviour you want from Dolly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9693f108",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dolly_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m print_final \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho was Dolly the sheep?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m answer \u001b[39m=\u001b[39m dolly_pipeline(\n\u001b[1;32m      9\u001b[0m     prompt,\n\u001b[1;32m     10\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m     11\u001b[0m     k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m     12\u001b[0m     output_length\u001b[39m=\u001b[39moutput_length,\n\u001b[1;32m     13\u001b[0m     print_live\u001b[39m=\u001b[39mprint_live,\n\u001b[1;32m     14\u001b[0m     print_final\u001b[39m=\u001b[39mprint_final,\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dolly_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "temperature = 0.6\n",
    "top_k = 5\n",
    "output_length = None\n",
    "print_live = True\n",
    "print_final = True\n",
    "\n",
    "prompt = \"Who was Dolly the sheep?\"\n",
    "answer = dolly_pipeline(\n",
    "    prompt,\n",
    "    temperature=temperature,\n",
    "    k=top_k,\n",
    "    output_length=output_length,\n",
    "    print_live=print_live,\n",
    "    print_final=print_final,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0a7a7cc",
   "metadata": {},
   "source": [
    "As Dolly is an instruction fine-tuned model, it was trained with a specific prompt format. Internally, the pipeline will transform your prompts into the correct format. To see the full prompt, you can view the `last_instruction_prompt` attribute on the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85d0409",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dolly_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(dolly_pipeline\u001b[39m.\u001b[39mlast_instruction_prompt[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dolly_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "print(dolly_pipeline.last_instruction_prompt[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2ca473c",
   "metadata": {},
   "source": [
    "Remember to detach your pipeline when you are finished to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e91b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dolly_pipeline.detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ece4041",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how you can easily run Dolly 2.0 for inference on Graphcore IPUs on text prompts. Instruction fine-tuning is a powerful method of turning a base LLM into one more suited for human interactivity, such as a question-answer model or a chatbot.\n",
    "\n",
    "Although larger instruction LLMs exist with more world knowledge such as ChatGPT, they are closed-source or are subject to non-commercial licensing. This makes Dolly 2.0 a significant milestone as the first of its kind, with future instruction fine-tuned LLMs no doubt to quickly follow. All of which will be truly open."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73e9821a",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Check out the full list of [IPU-powered Jupyter Notebooks](https://www.graphcore.ai/ipu-jupyter-notebooks) to get more of a feel for how IPUs perform on other tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
